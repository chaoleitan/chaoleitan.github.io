<!DOCTYPE HTML>
<html lang="en">
  <head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    	<title>Chaolei Tan</title>
    	<meta name="author" content="Chaolei Tan">
    	<meta name="viewport" content="width=device-width, initial-scale=1">
    	<link rel="stylesheet" type="text/css" href="stylesheet.css">
    	<link rel="icon" href="">
  </head>
	
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
        
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
        <td style="padding:2.5%;width:66.5%;vertical-align:middle">
        <p class="name" style="text-align: center;">
        Chaolei Tan
        </p>
        <p>I'm a first-year Ph.D. student at <a href="https://cse.hkust.edu.hk/">Dept. of Computer Science & Engineering (CSE)</a>, <a href="https://hkust.edu.hk/">The Hong Kong University of Science and Technology (HKUST)</a>,
	advised by <a href="https://zjuchenlong.github.io/">Prof. Long Chen</a>.
	I obtained my Master's degree from <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University (SYSU)</a> in 2024, where I was supervised by <a href="https://isee-ai.cn/~hujianfang/">Prof. Jian-Fang Hu </a> and <a href="https://www.isee-ai.cn/~zhwshi/">Prof. Wei-Shi Zheng</a>.
	Prior to that, I also received my Bachelor's degree from SYSU in 2021.
        </p>
        <p>
	My research interests lie in computer vision, machine learning and multimedia, with a special focus on developing models that can learn general and high-level knowledge about the world from multi-modality data
	like videos and language.
	</p>
        <p style="text-align:center">
        <a href="mailto:ctanak@cse.ust.hk">Email</a> &nbsp;/&nbsp;
        <a href="https://scholar.google.com/citations?user=DcvN3DgAAAAJ&hl=en">Scholar</a>
        </p>
        </td>
        <td style="padding:2.5%;width:40%;max-width:40%">
        <a href="images/photo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/photo_3.jpg" class="hoverZoomLink"></a>
        </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>News</h2>
        <p> <strong>2025-06:</strong> Two papers are accepted to ICCV 2025.
        <p> <strong>2024-12:</strong> Two papers are accepted to ACM MM 2024 and AAAI 2025.
        <p> <strong>2024-02:</strong> Two papers are accepted to CVPR 2024.
        <p> <strong>2023-02:</strong> Two papers are accepted to CVPR 2023.
        <p> <strong>2022-05:</strong> I win the championship of PIC Challenge HCVG Track at ACM MM 2022. 
        <p> <strong>2021-06:</strong> I receive the Best Paper Award of CVPR 2021 PIC Workshop.
        </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Selected Publications</h2>
        <p>* indicates equal contributions. See the full list in <a href="https://scholar.google.com/citations?user=DcvN3DgAAAAJ&hl=en">Google Scholar</a></p>
        </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
	<tr onmouseout="synop_stop()" onmouseover="synop_start()">
	<td style="padding:20px;width:40%;vertical-align:middle">
	<div class="one" style="width:100%">
	<div class="two" id='synop_image' style="width:100%">
	<video  width=100% muted autoplay loop><source src="images/synop.png" type="video/mp4">Your browser does not support the video tag.</video>
	</div>
	<img src='images/synop.png' width=100%>
	</div>
	<script type="text/javascript">
	  function synop_start() {
	    document.getElementById('synop_image').style.opacity = "1";
	  }
	
	  function synop_stop() {
	    document.getElementById('synop_image').style.opacity = "0";
	  }
	  synop_stop()
	</script>
	</td>
	<td style="padding:20px;width:60%;vertical-align:middle">
	<a href="">
	  <span class="papertitle">SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses</span>
	</a>
	<br>
	<strong>Chaolei Tan*</strong>, Zihang Lin*, Junfu Pu, Zhongang Qi, Wei-Yi Pei, Zhi Qu, Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu
	<br>
	<em>ACM International Conference on Multimedia (ACM MM)</em>, 2024
	<br>
	<a href="https://arxiv.org/abs/2408.01669">Paper</a>/
	<a href="https://synopground.github.io/">Website</a>
	<p>A large-scale video dataset with densely annotated paragraph timestamps to enable the new research direction of multi-paragraph video grounding on both long-form videos and long-term queries.</p>
	</td>
	</tr>
	
	<tr onmouseout="wsvpg_stop()" onmouseover="wsvpg_start()">
	<td style="padding:20px;width:40%;vertical-align:middle">
        <div class="one" style="width:100%">
	<div class="two" id='wsvpg_image' style="width:100%"><video  width=100% muted autoplay loop>
	<source src="images/wsvpg.png" type="video/mp4">Your browser does not support the video tag.</video>
	</div>
	<img src='images/wsvpg.png' width=100%>
        </div>
        <script type="text/javascript">
          function wsvpg_start() {
            document.getElementById('wsvpg_image').style.opacity = "1";
          }

          function wsvpg_stop() {
            document.getElementById('wsvpg_image').style.opacity = "0";
          }
          wsvpg_stop()
        </script>
	</td>
	<td style="padding:20px;width:60%;vertical-align:middle">
	<a href="https://arxiv.org/abs/2403.11463">
	<span class="papertitle">Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding</span>
        </a>
        <br>
	<strong>Chaolei Tan</strong>, Jianhuang Lai, Wei-Shi Zheng, Jian-Fang Hu
        <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
	<br>
	<a href="https://arxiv.org/abs/2403.11463">Paper</a>
        <p>First attempt to explore weakly-supervised setting of video paragraph grounding, where a siamese learning framework jontly conducting feature alignment and boundary regression is proposed.</p>
	</td>
	</tr>
	
	<tr onmouseout="hscnet_stop()" onmouseover="hscnet_start()">
	<td style="padding:20px;width:40%;vertical-align:middle">
        <div class="one" style="width:100%">
	<div class="two" id='hsnet_image' style="width:100%"><video  width=100% muted autoplay loop>
	<source src="images/hscnet.png" type="video/mp4">Your browser does not support the video tag.</video></div>
	<img src='images/hscnet.png' width=100%>
        </div>
        <script type="text/javascript">
          function hscnet_start() {
            document.getElementById('hscnet_image').style.opacity = "1";
          }

          function hscnet_stop() {
            document.getElementById('hscnet_image').style.opacity = "0";
          }
          hscnet_stop()
        </script>
	</td>
	<td style="padding:20px;width:60%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.html">
          <span class="papertitle">Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding</span>
        </a>
        <br>
	<strong>Chaolei Tan</strong>, Zihang Lin, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai
        <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023
	<br>
	<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.html">Paper</a>
        <p>Introducing hierarchical modeling into video paragraph grounding by hierarchically aligning semantic correspondence across videos and paragraphs for temporal decoding at multiple granularities.</p>
	</td>
	</tr>

	<tr onmouseout="referdino_stop()" onmouseover="referdino_start()">
 	<td style="padding:20px;width:40%;vertical-align:middle">
        <div class="one" style="width:100%">
	<div class="two" id='referdino_image' style="width:100%"><video  width=100% muted autoplay loop>
	<source src="images/referdino.png" type="video/mp4" width=100%>Your browser does not support the video tag.</video>
	</div>
	<img src='images/referdino.png' width=100%>
        </div>
        <script type="text/javascript">
          function referdino_start() {
            document.getElementById('referdino_image').style.opacity = "1";
          }

          function referdino_stop() {
            document.getElementById('referdino_image').style.opacity = "0";
          }
          referdino_stop()
        </script>
	</td>
 	<td style="padding:20px;width:60%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2501.14607">
          <span class="papertitle">ReferDINO: Referring Video Object Segmentation with Visual Grounding Foundations</span>
        </a>
        <br>
	Tianming Liang, Kun-Yu Lin, <strong>Chaolei Tan</strong>, Jianguo Zhang, Wei-Shi Zheng, Jian-Fang Hu
        <br>
        <em>International Conference on Computer Vision (ICCV)</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2501.14607">Paper</a>/
	<a href="https://isee-laboratory.github.io/ReferDINO/">Website</a>
        <p>Our first attempt to adapt pretrained visual grounding foundation models to Referring Video Object Segmentation (RVOS).</p>
	</td>
	</tr>
	
	<tr onmouseout="radi_stop()" onmouseover="radi_start()">
 	<td style="padding:20px;width:40%;vertical-align:middle">
        <div class="one" style="width:100%">
	<div class="two" id='radi_image' style="width:100%"><video  width=100% muted autoplay loop>
	<source src="images/radi.png" type="video/mp4" width=100%>Your browser does not support the video tag.</video>
	</div>
	<img src='images/radi.png' width=100%>
        </div>
        <script type="text/javascript">
          function radi_start() {
            document.getElementById('radi_image').style.opacity = "1";
          }

          function radi_stop() {
            document.getElementById('radi_image').style.opacity = "0";
          }
          radi_stop()
        </script>
	</td>
 	<td style="padding:20px;width:60%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2403.14430">
          <span class="papertitle">Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels</span>
        </a>
        <br>
	Tianming Liang, <strong>Chaolei Tan</strong>, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu
        <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2403.14430">Paper</a>
        <p>Tackle the incomplete annotation issues in open-ended video question answering with ranking distillation.</p>
	</td>
	</tr>
	
	<tr onmouseout="csdvl_stop()" onmouseover="csdvl_start()">
	<td style="padding:20px;width:40%;vertical-align:middle">
        <div class="one" style="width:100%">
	<div class="two" id='csdvl_image' style="width:100%"><video  width=100% muted autoplay loop><source src="images/csdvl.png" type="video/mp4">Your browser does not support the video tag.</video>
	</div>
        <img src='images/csdvl.png' width=100%>
        </div>
        <script type="text/javascript">
          function csdvl_start() {
            document.getElementById('csdvl_image').style.opacity = "1";
          }

          function csdvl_stop() {
            document.getElementById('csdvl_image').style.opacity = "0";
          }
          csdvl_stop()
        </script>
	</td>
	<td style="padding:20px;width:60%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.html">
          <span class="papertitle">Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding</span>
        </a>
        <br>
	Zihang Lin, <strong>Chaolei Tan</strong>, Jian-Fang Hu, Zhi Jin, Tiancai Ye, Wei-Shi Zheng
        <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023
        <br>
	<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.html">Paper</a>
        <p>To model the collaborative static and dynamic vision-language streams for better spatio-temporal video grounding.</p>
	</td>
	</tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
        <td>
        <h2>Education</h2>
        </td>
        </tr>
        </tbody></table>
	
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	<tr>
	<td style="padding:20px;width:40%;vertical-align:middle"><img src="images/hkust_blue_small.png"></td>
        <td width="60%" valign="center">
	<p>Ph.D. in Computer Science and Engineering</p>
	<p><a href="https://hkust.edu.hk/">HKUST</a>, Clear Water Bay</p>
        <p>Aug. 2024 - Present</p>
        <p>Advisor: <a href="https://zjuchenlong.github.io/">Prof. Long Chen</a></p>
        </td>
	</tr>
	
	<tr>
	<td style="padding:20px;width:40%;vertical-align:middle"><img src="images/sysu_small.png"></td>
        <td width="60%" valign="center">
	<p>M.Eng. in Computer Science and Technology</p>
	<p><a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>, Guangzhou</p>
        <p>Sep. 2021 - Jun. 2024</p>
        <p>Advisors: <a href="https://isee-ai.cn/~hujianfang/">Prof. Jian-Fang Hu </a> and <a href="https://www.isee-ai.cn/~zhwshi/">Prof. Wei-Shi Zheng</a></p>
        </td>
	</tr>

	<tr>
	<td style="padding:20px;width:40%;vertical-align:middle"><img src="images/sysu_small.png"></td>
        <td width="60%" valign="center">
	<p>B.Eng. in Telecommunication Engineering</p>
	<p><a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>, Guangzhou</p>
        <p>Sep. 2017 - Jun. 2021</p>
        <p>GPA: 4.12/5.0, Ranking: 2/81</p>
        </td>
	</tr>
	</tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
        <td>
        <h2>Experiences</h2>
        </td>
        </tr>
        </tbody></table>
	
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	<tr>
	<td style="padding:20px;width:40%;vertical-align:middle"><img src="images/arc_small.png"></td>
        <td width="60%" valign="center">
	<p>Research Intern in Computer Vision</p>
	<p><a href="https://arc.tencent.com/en/index">Tencent ARC Lab</a>, Shenzhen</p>
        <p>Jul. 2023 - Jun. 2024</p>
        <p>Mentors: <a href="https://pujunfu.github.io/">Dr. Junfu Pu </a>and <a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ&hl=en">Dr. Zhongang Qi</a></p>
        </td>
	</tr>

	<tr>
	<td style="padding:20px;width:40%;vertical-align:middle"><img src="images/wechat_green_small.png"></td>
        <td width="60%" valign="center">
	<p>Research Intern in Computer Vision</p>
	<p><a href="https://www.wechat.com/en/">Tencent WeChat</a>, Guangzhou</p>
        <p>Jul. 2021 - Jan. 2023</p>
        <p>Mentor: <a href="https://github.com/tcye">Mr. Tiancai Ye</a></p>
        </td>
	</tr>
	</tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Honors and Awards</h2>
	<ul>
	<li>HKUST RedBird PhD Scholarship</li>
	<li>ACM MM 2024 Outstanding Reviewer</li>
	<li>CSIG Outstanding Master's Dissertation Honorable Mention Award</li>
	<li>SYSU Outstanding Master's Dissertation Award</li>
	<li>ACM MM 2022 PIC Challenge HCVG Track 1st Place Award</li>
	<li>CVPR 2021 PIC Workshop Best Paper Award</li>
	<li>SYSU Guangdong Guangda Further Study Scholarship</li>
	<li>Guangdong Soong Ching-ling Scholarship</li>
	</ul>
        </td>
        </tr>
	</tbody></table>

        <tr>
        <td style="width:25%;vertical-align:middle">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=O3-lmMfORWD5pfQ3VmeCZU1tdQOWTICS5FHwHMBRWCc&cl=ffffff&w=a"></script>
        </td>
        </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	</tbody></table>
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
	<td style="padding:0px">
	<br>
	<p style="text-align:right;font-size:small;">
	This website is built on top of the awesome <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron template</a>.
	</p>
	</td>
	</tr>
	</tbody></table>
	
        </td>
      </tr>
    </table>
  </body>
</html>
