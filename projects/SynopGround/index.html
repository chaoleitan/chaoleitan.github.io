<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SynopGround">
  <meta name="keywords" content="multi-paragraph video grounding dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SynopGround</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/drama_logo.png?">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span vertical-align="middle">SynopGround:</span></h1>
<!--             <h1 class="title is-1 publication-title"><img id="logo" vertical-align="middle" width="4.5%" src="images/drama_logo.png"><span vertical-align="middle">SynopGround:</span></h1> -->
            <h1 class="title is-1 publication-title">A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses</h1>
            <h3 class="subtitle is-4 publication-awards">ACM MM 2024</h3>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                <a href="https://chaoleitan.github.io/" style="color:#f68946;font-weight:normal;">Chaolei Tan<sup>*&sect12</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://zanglam.github.io/" style="color:#008AD7;font-weight:normal;">Zihang Lin<sup>*&sect1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://pujunfu.github.io/" style="color:#F2A900;font-weight:normal;">Junfu Pu<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ&hl=en&oi=ao" style="color:#f68946;font-weight:normal;">Zhongang Qi<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=lfEDnb8AAAAJ&hl=zh-CN" style="color:#f68946;font-weight:normal;">Wei-Yi Pei<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Zhi_Qu3" style="color:#f68946;font-weight:normal;">Zhi Qu<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://dblp.org/pid/51/2047.html" style="color:#f68946;font-weight:normal;">Yexin Wang<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/YingShanProfile/" style="color:#f68946;font-weight:normal;">Ying Shan<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.isee-ai.cn/~zhwshi/" style="color:#f68946;font-weight:normal;">Wei-Shi Zheng<sup>1</sup></a>
              </span>
              <span class="author-block">
                <a href="https://isee-ai.cn/~hujianfang/" style="color:#f68946;font-weight:normal;">Jian-Fang Hu<sup>&dagger;1</sup></a>
              </span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup>1</sup>Sun Yat-sen University</b></span>
              <span class="author-block"><sup>2</sup>ARC Lab, Tencent PCG</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
              <span class="author-block"><b>&dagger;</b> Corresponding author</span>
            </div>

            <div class="is-size-6 publication-authors">
                <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Tencent ARC Lab.</span>
            </div>
  
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset & Code</span>
                  </a>
                </span>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero teaser">
  <div class="container is-max-desktop">
  <div class="hero-body">
  <h4 class="subtitle has-text-centered">
  ðŸ”¥<span style="color: #ff3860"></span> SynopGround is the first video grounding dataset characterized by both long-form video and long-term query inputs, which has ~100 words per query and ~43 minutes per video on average.
  <br><br>
  It brings unique challenges in modeling the long-term video-language relationships.  In comparison to existing datasets that focus on traditional short and simple queries, our SynopGround introduces long complex paragraphs as language queries into video grounidng for the first time,
  which benefits the model to learn unambiguous cross-modal correspondence and the ability to handle complicated long-term contexts.
  </h4>
  </div>
  </div>
  </section>
  
  <section class="section"  style="background-color:#efeff081" id="Abstract">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths">
  <h2 class="title is-3">Abstract</h2>
  <div class="content has-text-justified">
  <p>
  Video grounding is a fundamental problem in vision-language understanding, which aims to localize the natural language queries in an untrimmed video. However, current video grounding datasets merely focus on the multimodal understanding of simple events and are either limited to shorter videos or brief sentences, which hinders the model from evolving toward stronger multimodal understanding capabilities or being applied in some more complex downstream scenarios.
  To address these limitations, we present a large-scale video grounding dataset named SynopGround, in which more than 2800 hours of videos are sourced from popular TV dramas and are paired with accurately localized human-written synopses. Each paragraph in the synopsis serves as a language query and is manually annotated with precise temporal boundaries in the long video. These paragraph queries are tightly correlated to each other and contain a wealth of abstract expressions summarizing video storylines and specific descriptions portraying event details, which enables the model to learn multimodal perception on more intricate concepts over longer context dependencies.
  Based on the dataset, we further introduce a more complex setting of video grounding dubbed Multi-Paragraph Video Grounding (MPVG), which takes as input multiple paragraphs and a long video for grounding each paragraph query to its temporal interval. In addition, we propose a novel Local-Global Multimodal Reasoner (LGMR) to explicitly model the local-global structures of long-term multimodal inputs and iteratively conduct fine-grained cross-modal reasoning within and across the two levels of structures between the long videos and long paragraphs.
  Our method provides an effective baseline solution to the multi-paragraph video grounding problem. Extensive experiments verify the proposed model's effectiveness as well as its superiority in long-term multi-paragraph video grounding over prior state-of-the-arts.
  </p>
  </div>
  </div>
  </div>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Introduction</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/intro.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  Currently, most of the commonly-used datasets are based on short videos and brief sentence queries. This setup limits the model in developing stronger abilities like long-term contextual multimodal understanding that bridges long-form videos and long-text queries. Besides, shorter queries that describe detailed events, are more prone to causing the risk of semantic ambiguity in referring expressions, i.e., the occurrence of one-to-many correspondence between queries and moments, which will adversely affect the model learning.
  In this work, we curate and present a large-scale dataset called SynopGround, in which over 2800 hours of narrative videos with human-written synopses are manually annotated with dense timestamps.
  Different from the short general descriptions like ``She steps closer.'' that are widely used in previous datasets, we use synopses involving both high-level expressions conveying abstract concepts and concrete descriptions picturing specific details. As shown in table above, there are very concrete descriptions for visible activity concepts like ``go to the cabin'', as well as extremely concise and abstract expressions like ``spent a happy time'' in the query from our dataset. Such language queries are more challenging, more unambiguous and can enforece the model to learn long-term cross-modal reasoning over higher-level concepts and storylines.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>
  
  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Dataset Visualization</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/dataset_sample.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>Multi-Paragraph Video Grounding (MPVG) and two representative dataset samples.</b>
  Given a video and a synopsis <i>Q</i> that contains <i>N</i> paragraphs {<i>Q<sup>1</sup></i>, <i>Q<sup>2</sup></i>, ..., <i>Q<sup>N</sup></i>}, the model should predict the corresponding temporal interval 
  for each paragraph <i>Q<sup>i</sup></i> in the form of starting and ending time.
  It is much more complex and requires to understand both short-term intra-paragraph semantics and long-term inter-paragraph dependencies for video grounding, which connects the complex temporal structures of long videos with the complicated semantics of long paragraphs.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Data Distribution</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/data_distribution.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>(a): Genre distribution of TV dramas. (b): Normalized duration of target video segments. (c): Number of queries per video. (d): Normalized start timestamp distribution. (e): Normalized end timestamp distribution.</b>
  As shown in figure (a) above, the TV dramas used in our dataset cover a wide spectrum of genres, which demonstrates the diversity of the collected data. In figure (b) above, we show the normalized duration of the target video segments. Most of the target video segments cover less than 20% of the full video, which can be challenging for the model to correctly localize.
  In figure (c) above, we visualize the distribution of the number of queries/paragraphs in each synopsis, and most synopses are composed of 5-13 paragraphs. Exploring the contextual information among these paragraphs is important for achieving promising performance in our multi-paragraph video grounding task.
  In figure (d) and (e) above, we visualize the temporal distributions of the starting timestamps and ending timestamps of the target video segments. Both of them approximately present a uniform distribution, which ensures the model cannot benefit much from the distribution bias.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Detailed Statistical Comparison with Previous Video Grounding Datasets</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/statistics.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>Comprehensive comparisons of our SynopGround with the existing video grounding datasets.</b>
  As shown, the videos in our dataset are much longer than those in Charades-STA, ActivityNet-Captions, DiDeMo, TACoS and Ego4d-NLQ. Although the average video duration in our dataset is shorter than that of MAD, our total duration of videos is more than twice that of MAD, showing that our dataset is at a large scale.
  Furthermore, the duration of target segments in our dataset is significantly longer. This requires the model to capture the full picture of the holistic story conveyed in the language queries and the video, which is challenging.
  Besides, our dataset is the first benchmark to introduce paragraph queries, and the average number of words in each query is significantly larger than that of other datasets, which greatly reduces the semantic ambiguity of the queries and brings more challenges to the model's cross-modal understandinging abilities.
  Moreover, our synopsis queries involve both abstract expressions and concrete descriptions, enabling the model to learn semantic concepts at more diverse abstraction levels.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Baseline Method: Local-Global Multimodal Reasoner (LGMR)</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/baseline_method.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>Overview of our proposed Local-Global Multimodal Reasoner (LGMR).</b>
  It consists of a local-global temporal encoder for structural long-term temporal modeling and a local-global iterative decoder to adaptively extract the local subparagraph features guided by global paragraph semantics and reason through the local and global semantics for multi-paragraph video grounding.
  The video encoder decomposes the temporal correlations of long videos into intra-window and inter-window parts for efficient long-term temporal modeling. The query decoder first extracts subparagraph representations with a set of learnable queries guided by the global semantics of paragraphs, and then repeatedly conducts cross-modal reasoning within and across the local and global queries.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Experimental Results</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/experimental_results.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  As shown above, we evaluate the performance of our proposed LGMR on the challenging multi-paragraph video grounding task and compare it with the existing state-of-the-art methods DepNet and PRVG. The comparison results demonstrate that our proposed model achieves the best performance and outperforms others by a significant margin, which validates the effectiveness of the proposed LGMR method for addressing MPVG.
  In addition, we conduct detailed experiments to verify our proposed idea to model and reason the local-global structures of long queries. First, the model using only local queries for cross-modal decoding process achieves a significantly lower performance than our final model.
  Secondly, we observe significant gains in performance when jointly modeling the local and global queries from the long text inputs during decoding, showing the importance of our local-global query modeling.
  In addition, we find that utilizing a cross-level interaction module can even further improve the performance, which suggests the benefits of mining and reasoning the complementary local-global information.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>
  @inproceedings{
  tan2024synopground,
  author={Tan, Chaolei and Lin, Zihang and Pu, Junfu and Qi, Zhongang and Pei, Wei-Yi and Qu, Zhi and Wang, Yexin and Shan, Ying and Zheng, Wei-Shi and Hu, Jian-Fang},
  title={SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses},
  booktitle={ACM MM},
  year={2024}
  }
  </code></pre>
  </div>
  </section>
  
  <section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
  <h2 class="title">Acknowledgement</h2>
  <p>
  This website is adapted from <a
  href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
  Commons Attribution-ShareAlike 4.0 International License</a>.
  </p>

  <p>
  <b>Usage and License Notices</b>: The dataset and code are intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
  </p>
    
  </div>
  </section>

</body>

</html>
