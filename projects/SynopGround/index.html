<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SynopGround">
  <meta name="keywords" content="multi-paragraph video grounding dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SynopGround</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/drama_logo.png?">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span vertical-align="middle">SynopGround:</span></h1>
<!--             <h1 class="title is-1 publication-title"><img id="logo" vertical-align="middle" width="4.5%" src="images/drama_logo.png"><span vertical-align="middle">SynopGround:</span></h1> -->
            <h1 class="title is-1 publication-title">A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses</h1>
            <h3 class="subtitle is-4 publication-awards">ACM MM 2024</h3>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                <a href="https://chaoleitan.github.io/" style="color:#f68946;font-weight:normal;">Chaolei Tan<sup>*&sect12</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://zanglam.github.io/" style="color:#008AD7;font-weight:normal;">Zihang Lin<sup>*&sect1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://pujunfu.github.io/" style="color:#F2A900;font-weight:normal;">Junfu Pu<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ&hl=en&oi=ao" style="color:#f68946;font-weight:normal;">Zhongang Qi<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=lfEDnb8AAAAJ&hl=zh-CN" style="color:#f68946;font-weight:normal;">Wei-Yi Pei<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Zhi_Qu3" style="color:#f68946;font-weight:normal;">Zhi Qu<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://dblp.org/pid/51/2047.html" style="color:#f68946;font-weight:normal;">Yexin Wang<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/YingShanProfile/" style="color:#f68946;font-weight:normal;">Ying Shan<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.isee-ai.cn/~zhwshi/" style="color:#f68946;font-weight:normal;">Wei-Shi Zheng<sup>1</sup></a>
              </span>
              <span class="author-block">
                <a href="https://isee-ai.cn/~hujianfang/" style="color:#f68946;font-weight:normal;">Jian-Fang Hu<sup>&dagger;1</sup></a>
              </span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup>1</sup>Sun Yat-sen University</b></span>
              <span class="author-block"><sup>2</sup>ARC Lab, Tencent PCG</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
              <span class="author-block"><b>&dagger;</b> Corresponding author</span>
            </div>

            <div class="is-size-6 publication-authors">
                <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Tencent ARC Lab.</span>
            </div>
  
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset & Code</span>
                  </a>
                </span>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero teaser">
  <div class="container is-max-desktop">
  <div class="hero-body">
  <h4 class="subtitle has-text-centered">
  ðŸ”¥<span style="color: #ff3860"></span> LLaVA-1.5 achieves SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods that use billion-scale data.
  <br><br>
  LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna
  for general-purpose visual and language understanding,
  achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.
  </h4>
  </div>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">SynopGround: Video Grounding with both long-form videos and long-term queries.</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/intro.png" alt="neti" width="100%"/>
  <br>
  (Left) SynopGround is characterized by its both long-term video and query inputs, i.e., ~100 words per query and ~43 minutes per video on average, which brings unique challenges in modeling the long-term video-language relationships. 
  <br>
  (Right) In comparison to existing datasets that focus on traditional short and simple queries, our SynopGround introduces long complex paragraphs as language queries into video grounidng for the first time,
  which benefits the model to learn unambiguous cross-modal correspondence and the ability to handle complicated long-term contexts.
  </div>
  </p>
  </div>
  </section>
  
  <section class="section"  style="background-color:#efeff081" id="Abstract">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths">
  <h2 class="title is-3">Abstract</h2>
  <div class="content has-text-justified">
  <p>
  Video grounding is a fundamental problem in vision-language understanding, which aims to localize the natural language queries in an untrimmed video. However, current video grounding datasets merely focus on the multimodal understanding of simple events and are either limited to shorter videos or brief sentences, which hinders the model from evolving toward stronger multimodal understanding capabilities or being applied in some more complex downstream scenarios.
  To address these limitations, we present a large-scale video grounding dataset named SynopGround, in which more than 2800 hours of videos are sourced from popular TV dramas and are paired with accurately localized human-written synopses. Each paragraph in the synopsis serves as a language query and is manually annotated with precise temporal boundaries in the long video. These paragraph queries are tightly correlated to each other and contain a wealth of abstract expressions summarizing video storylines and specific descriptions portraying event details, which enables the model to learn multimodal perception on more intricate concepts over longer context dependencies.
  Based on the dataset, we further introduce a more complex setting of video grounding dubbed Multi-Paragraph Video Grounding (MPVG), which takes as input multiple paragraphs and a long video for grounding each paragraph query to its temporal interval. In addition, we propose a novel Local-Global Multimodal Reasoner (LGMR) to explicitly model the local-global structures of long-term multimodal inputs and iteratively conduct fine-grained cross-modal reasoning within and across the two levels of structures between the long videos and long paragraphs.
  Our method provides an effective baseline solution to the multi-paragraph video grounding problem. Extensive experiments verify the proposed model's effectiveness as well as its superiority in long-term multi-paragraph video grounding over prior state-of-the-arts.
  </p>
  </div>
  </div>
  </div>
  </div>
  </section>
  
  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
<!--   <h2 class="title is-3">Our Meta-Personalized VLM</h2> -->
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/dataset_sample.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>The proposed Multi-Paragraph Video Grounding (MPVG) setting and representative video samples in our SynopGround:</b>
  Given a video and a synopsis <i>Q</i> that contains <i>N</i> paragraphs {<i>Q<sup>1</sup></i>, <i>Q<sup>2</sup></i>, ..., <i>Q<sup>N</sup></i>}, the model should predict the corresponding temporal interval 
  for each paragraph <i>Q<sup>i</sup></i> in the form of starting and ending time.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Data Distribution</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/data_distribution.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>(a): Genre distribution of TV dramas. (b): Normalized duration of target video segments. (c): Number of queries per video. (d): Normalized start timestamp distribution. (e): Normalized end timestamp distribution.</b>
  As shown in figure (a) above, the TV dramas used in our dataset cover a wide spectrum of genres, which demonstrates the diversity of the collected data. In figure (b) above, we show the normalized duration of the target video segments. Most of the target video segments cover less than 20% of the full video, which can be challenging for the model to correctly localize.
  In figure (c) above, we visualize the distribution of the number of queries/paragraphs in each synopsis, and most synopses are composed of 5-13 paragraphs. Exploring the contextual information among these paragraphs is important for achieving promising performance in our multi-paragraph video grounding task.
  In figure (d) and (e) above, we visualize the temporal distributions of the starting timestamps and ending timestamps of the target video segments. Both of them approximately present a uniform distribution, which ensures the model cannot benefit much from the distribution bias.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Detailed Statistical Comparison with Previous Video Grounding Datasets</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/statistics.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  As shown, the videos in our dataset are much longer than those in Charades-STA, ActivityNet-Captions, DiDeMo, TACoS and Ego4d-NLQ. Although the average video duration in our dataset is shorter than that of MAD, our total duration of videos is more than twice that of MAD, showing that our dataset is at a large scale.
  Furthermore, the duration of target segments in our dataset is significantly longer. This requires the model to capture the full picture of the holistic story conveyed in the language queries and the video, which is challenging. Besides, our dataset is the first benchmark to introduce paragraph queries, and the average number of words in each query is significantly larger than that of other datasets, which greatly reduces the semantic ambiguity of the queries.
  Moreover, our synopsis queries involve both abstract expressions and concrete descriptions, enabling the model to learn semantic concepts at more diverse abstraction levels.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
  <h2 class="title is-3">Our Proposed Baseline Method for MPVG: Local-Global Multimodal Reasoner</h2>
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/baselin_method.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>The proposed Multi-Paragraph Video Grounding (MPVG) setting and representative video samples in our SynopGround:</b>
  Given a video and a synopsis <i>Q</i> that contains <i>N</i> paragraphs {<i>Q<sup>1</sup></i>, <i>Q<sup>2</sup></i>, ..., <i>Q<sup>N</sup></i>}, the model should predict the corresponding temporal interval 
  for each paragraph <i>Q<sup>i</sup></i> in the form of starting and ending time.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>

  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>
  @inproceedings{
  tan2024synopground,
  author={Tan, Chaolei and Lin, Zihang and Pu, Junfu and Qi, Zhongang and Pei, Wei-Yi and Qu, Zhi and Wang, Yexin and Shan, Ying and Zheng, Wei-Shi and Hu, Jian-Fang},
  title={SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses},
  booktitle={ACM MM},
  year={2024}
  }
  </code></pre>
  </div>
  </section>
  
  <section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
  <h2 class="title">Acknowledgement</h2>
  <p>
  This website is adapted from <a
  href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
  Commons Attribution-ShareAlike 4.0 International License</a>.
  </p>

  <p>
  <b>Usage and License Notices</b>: The dataset and code are intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
  </p>
    
  </div>
  </section>

</body>

</html>
