<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SynopGround">
  <meta name="keywords" content="multi-paragraph video grounding dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SynopGround</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/drama_logo.png?">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img id="logo" vertical-align="middle" width="5.5%" src="images/drama_logo.png"><span vertical-align="middle">SynopGround:</span></h1>
            <h1 class="title is-1 publication-title">A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses</h1>
            <h3 class="subtitle is-4 publication-awards">ACM MM 2024</h3>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                <a href="https://chaoleitan.github.io/" style="color:#f68946;font-weight:normal;">Chaolei Tan<sup>*&sect12</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://zanglam.github.io/" style="color:#008AD7;font-weight:normal;">Zihang Lin<sup>*&sect1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://pujunfu.github.io/" style="color:#F2A900;font-weight:normal;">Junfu Pu<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ&hl=en&oi=ao" style="color:#f68946;font-weight:normal;">Zhongang Qi<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=lfEDnb8AAAAJ&hl=zh-CN" style="color:#f68946;font-weight:normal;">Wei-Yi Pei<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Zhi_Qu3" style="color:#f68946;font-weight:normal;">Zhi Qu<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://dblp.org/pid/51/2047.html" style="color:#f68946;font-weight:normal;">Yexin Wang<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/YingShanProfile/" style="color:#f68946;font-weight:normal;">Ying Shan<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.isee-ai.cn/~zhwshi/" style="color:#f68946;font-weight:normal;">Wei-Shi Zheng<sup>1</sup></a>
              </span>
              <span class="author-block">
                <a href="https://isee-ai.cn/~hujianfang/" style="color:#f68946;font-weight:normal;">Jian-Fang Hu<sup>&dagger;1</sup></a>
              </span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup>1</sup>Sun Yat-sen University</b></span>
              <span class="author-block"><sup>2</sup>ARC Lab, Tencent PCG</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
              <span class="author-block"><b>&dagger;</b> Corresponding author</span>
            </div>

            <div class="is-size-6 publication-authors">
                <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Tencent ARC Lab.</span>
            </div>
            
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset & Code</span>
                  </a>
                </span>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column">
<!--   <h2 class="title is-3">Our Meta-Personalized VLM</h2> -->
  <div class="content has-text-justified">
  </div>
  <div class="column is-centered has-text-centered">
  <img src="images/dataset_sample.png" alt="neti" width="100%"/>
  </div>
  <ul class="column is-centered has-text-justified">
  <p>
  <b>The proposed Multi-Paragraph Video Grounding (MPVG) setting and representative video samples in our SynopGround:</b>
  Given a video and a synopsis <i>Q</i> that contains <i>N</i> paragraphs {<i>Q<sup>1</sup></i>, <i>Q<sup>2</sup></i>, ..., <i>Q<sup>N</sup></i>}, the model should predict the corresponding temporal interval 
  for each paragraph <i>Q<sup>i</sup></i> in the form of starting and ending time.
  </p>
  </ul>
  </div>
  </div>
  </p>
  </div>
  </section>
  
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          ðŸ”¥<span style="color: #ff3860"></span> LLaVA-1.5 achieves SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods that use billion-scale data.
          <br><br>
          LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna
          for general-purpose visual and language understanding,
          achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.
        </h4>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>
  @inproceedings{
  tan2024synopground,
  author={Tan, Chaolei and Lin, Zihang and Pu, Junfu and Qi, Zhongang and Pei, Wei-Yi and Qu, Zhi and Wang, Yexin and Shan, Ying and Zheng, Wei-Shi and Hu, Jian-Fang},
  title={SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses},
  booktitle={ACM MM},
  year={2024}
  }
  </code></pre>
  </div>
  </section>
  
  <section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
  <h2 class="title">Acknowledgement</h2>
  <p>
  This website is adapted from <a
  href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
  Commons Attribution-ShareAlike 4.0 International License</a>.
  </p>

  <p>
  <b>Usage and License Notices</b>: The dataset and code are intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
  </p>
    
  </div>
  </section>

</body>

</html>
